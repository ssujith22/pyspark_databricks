{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "301934c1-7af5-4175-a36b-383fdc806961",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#import\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#create spark session\n",
    "Spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"app\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "673a0055-2c08-4596-b244-874c4568f291",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "#create PySpark RDD from Parallelize\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6])\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b102afa-a432-4ec0-9e36-ba5b7182c417",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Java', 20000), ('Python', 10000), ('Scala', 30000)]\n"
     ]
    }
   ],
   "source": [
    "#create PySpark RDD from Tuple\n",
    "data = [(\"Java\",20000),(\"Python\",10000),(\"Scala\",30000)]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(rdd.collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5e0b8a-476c-45b2-ac2e-45d826c0ecd1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# create RDD from range function\n",
    "rddRange = spark.sparkContext.parallelize(range(1,6))\n",
    "print(rddRange.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9652e1-f478-49df-b951-300022ca1f8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "#create RDD from another RDD\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])\n",
    "new_rdd = rdd.map(lambda x: x*2)\n",
    "print(new_rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62866f80-ea07-4be8-a141-b3ec0adf8fdc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Kumar', 'age': 39, 'city': 'New York'}]\n"
     ]
    }
   ],
   "source": [
    "# import JSON\n",
    "import json\n",
    "\n",
    "#create RDD from JSON\n",
    "# spark.sparkContext = sc we can use sc short\n",
    "json_data = '{\"name\":\"Kumar\",\"age\":39,\"city\":\"New York\"}'\n",
    "rdd_json = sc.parallelize([json.loads(json_data)])\n",
    "print(rdd_json.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b24390e-3ca6-4c6e-bb25-ccd30d52aa9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n|  name|salary|\n+------+------+\n| James|  3000|\n|  Anna|  4001|\n|Robert|  6200|\n+------+------+\n\n[Row(name='James', salary=3000), Row(name='Anna', salary=4001), Row(name='Robert', salary=6200)]\n"
     ]
    }
   ],
   "source": [
    "data = [('James',3000),('Anna',4001),('Robert',6200)]\n",
    "df = spark.createDataFrame(data,[\"name\",\"salary\"])\n",
    "df.show()\n",
    "\n",
    "#converts DataFrame to rdd\n",
    "rdd = df.rdd\n",
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "317639ee-670a-4560-9611-925d9242f03d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----------+-------------------+\n|  a|  b|  c|         d|                  e|\n+---+---+---+----------+-------------------+\n|  1|4.0| aa|2000-08-01|2000-08-01 12:00:00|\n|  2|8.0| bb|2000-06-02|2000-06-02 12:00:00|\n|  3|5.0| cc|2000-05-03|2000-05-03 12:00:00|\n+---+---+---+----------+-------------------+\n\nroot\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime,date\n",
    "import pandas as pd\n",
    "#need to import for session creation\n",
    "from pyspark.sql import SparkSession,Row\n",
    "\n",
    "#pyspark dataframe\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1,4.,'aa',date(2000,8,1), datetime(2000,8,1,12,0)),\n",
    "    (2,8.,'bb',date(2000,6,2), datetime(2000,6,2,12,0)),\n",
    "    (3,5.,'cc',date(2000,5,3), datetime(2000,5,3,12,0))\n",
    "])\n",
    "\n",
    "df =spark.createDataFrame(rdd,schema=['a','b','c','d','e'])\n",
    "\n",
    "# show table\n",
    "df.show()\n",
    "\n",
    "# show schema\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fee64c4-3c5d-4311-8eb6-f63bb7c8729f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-----+----------+-------------------+\n|  a|  b|    c|         d|                  e|\n+---+---+-----+----------+-------------------+\n|  1|4.0|rank1|2000-08-01|2000-08-01 12:00:00|\n|  2|8.0|rank2|2000-06-02|2000-06-02 12:00:00|\n|  4|5.0|rank3|2000-05-03|2000-05-03 12:00:00|\n+---+---+-----+----------+-------------------+\n\nroot\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#need to import for session creation\n",
    "from pyspark.sql import SparkSession,Row\n",
    "\n",
    "# creating the session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# schema creation by passing list\n",
    "df=spark.createDataFrame([\n",
    "     Row (a=1, b=4., c='rank1',d=date(2000,8,1),\n",
    "        e=datetime(2000,8,1,12,0)),\n",
    "     Row (a=2, b=8., c='rank2',d=date(2000,6,2),\n",
    "        e=datetime(2000,6,2,12,0)),\n",
    "     Row (a=4, b=5., c='rank3',d=date(2000,5,3),\n",
    "        e=datetime(2000,5,3,12,0))\n",
    "],schema=['a','b','c','d','e'])\n",
    "\n",
    "\n",
    "# show table\n",
    "df.show()\n",
    "\n",
    "# show schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0113c456-c4d0-4f4a-be95-c230104c44de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+----------+-------------------+\n|  a|  b|  c|         d|                  e|\n+---+---+---+----------+-------------------+\n|  1|4.0| aa|2000-08-01|2000-08-01 12:00:00|\n|  2|8.0| bb|2000-06-02|2000-06-02 12:00:00|\n|  3|5.0| cc|2000-05-03|2000-05-03 12:00:00|\n+---+---+---+----------+-------------------+\n\nroot\n |-- a: long (nullable = true)\n |-- b: double (nullable = true)\n |-- c: string (nullable = true)\n |-- d: date (nullable = true)\n |-- e: timestamp (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime,date\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# creating the session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#PySpark DataFrame from a pandas DataFrame\n",
    "pandas_df =pd.DataFrame({\n",
    "    'a' : [1,2,3],\n",
    "\n",
    "    'b' : [4.,8.,5.],\n",
    "\n",
    "    'c' : ['aa','bb','cc'],\n",
    "\n",
    "    'd' : [date(2000,8,1), date(2000,6,2), date(2000,5,3)],\n",
    "\n",
    "    'e' : [datetime(2000,8,1,12,0),\n",
    "           datetime(2000,6,2,12,0),\n",
    "           datetime(2000,5,3,12,0)]\n",
    "})\n",
    "\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df\n",
    "\n",
    "#show table\n",
    "df.show()\n",
    "\n",
    "# show schema\n",
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Pyspark 2024-08-21 09:45:27",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
